{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils \n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchsummary import summary\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(project=\"test_20220707\", id='LSTM_res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SpiralDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, nTime=72, nBatch=128, seed=0):\n",
    "        x = utils.SynthData.spiral(nTime, nBatch, seed)\n",
    "        x_, y_ = utils.DataProcessing.cart2pol(x[:, :, 0], x[:, :, 1])\n",
    "        x_ = repeat(x_, 'b t -> b t f', f=1)\n",
    "        y_ = repeat(y_, 'b t -> b t f', f=1)\n",
    "        x = np.concatenate([x, x_, y_], axis=2)\n",
    "        self.data = x\n",
    "              \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx, :, :]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    \n",
    "dataset_train = SpiralDataset(nTime=72, nBatch=128, seed=0)\n",
    "dataset_val = SpiralDataset(nTime=72, nBatch=128, seed=1)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmLayer(nn.Module):\n",
    "    def __init__(self, nFeature=32, dropout=0.1):\n",
    "        super(LstmLayer, self).__init__()        \n",
    "        self.lstm = nn.LSTM(nFeature, nFeature, 1, batch_first=True)\n",
    "        self.linear = nn.Linear(nFeature*2, nFeature)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "                \n",
    "    def forward(self, x, hc=None):   \n",
    "        # b t f \n",
    "        res = x\n",
    "        if hc is None:\n",
    "            x, _ = self.lstm(x)\n",
    "        else:\n",
    "            x, _ = self.lstm(x, hc)\n",
    "        x = rearrange([x, res], 'c b t f -> b t (f c)')\n",
    "        x = self.linear(x)\n",
    "        x = F.silu(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Lstm(nn.Module):\n",
    "    def __init__(self, nLayer=8, nFeature=16, dropout=0.1):\n",
    "        super(Lstm, self).__init__()\n",
    "        self.seq = nn.Sequential()\n",
    "        for i in range(nLayer):\n",
    "            self.seq.add_module(f'lstm{i}', LstmLayer(nFeature=nFeature))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.seq(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class TrajNet(nn.Module):\n",
    "    def __init__(self, nHidden=8, nFeature=16, num_layers=4, dropout=0.1):\n",
    "        super(TrajNet, self).__init__()\n",
    "        self.nhidden = nHidden\n",
    "        self.nFeature = nFeature\n",
    "\n",
    "        # encoding\n",
    "        self.enc_conv = nn.Conv1d(4, nFeature, 1)\n",
    "        self.encoder = Lstm(nLayer=num_layers, nFeature=nFeature, dropout=dropout)\n",
    "        \n",
    "        # hidden\n",
    "        self.hidden = nn.Linear(nHidden, nHidden)\n",
    "        # self.silu = nn.SiLU()\n",
    "        # self.mu = nn.Linear(nFeature, nHidden)\n",
    "        # self.log_var = nn.Linear(nFeature, nHidden)\n",
    "        # self.alpha = torch.tensor(0.01)\n",
    "                \n",
    "        \n",
    "        # decode\n",
    "        self.decoder = Lstm(nLayer=num_layers, nFeature=nFeature, dropout=dropout)\n",
    "        self.dec_conv = nn.Conv1d(nFeature, 2, 1)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def positionEncoding(x):\n",
    "        # x: b t f \n",
    "        nBatch = x.shape[0]\n",
    "        nTime = x.shape[1]\n",
    "        p = torch.arange(0, nTime).type_as(x)\n",
    "        p = p / 300\n",
    "        p = repeat(p, 't -> b t f', b=nBatch, f=1)\n",
    "        x = torch.concat([x, p], dim=2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def reparameterize(mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)  # standard deviation\n",
    "        sample = torch.normal(mu, std).type_as(mu) # b f\n",
    "        return sample\n",
    "    \n",
    "    @staticmethod\n",
    "    def kl_loss_fun(mu, log_var):\n",
    "        return (-0.5 * (1 + log_var - mu**2 - torch.exp(log_var)).sum(dim=1)).mean(dim=0)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: b t f \n",
    "        nBatch = x.shape[0]\n",
    "        nTime = x.shape[1]\n",
    "        \n",
    "        # --------------------------------- encoding --------------------------------- #\n",
    "        x = rearrange(x, 'b t f -> b f t')\n",
    "        x = self.enc_conv(x)\n",
    "        x = rearrange(x, 'b f t -> b t f')\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # ----------------------------- hidden bottleneck ---------------------------- #\n",
    "        x = x[:, -1, :]\n",
    "        # hidden = self.hidden(hidden)\n",
    "        # hidden = torch.tanh(hidden)\n",
    "        # hidden = self.silu(hidden)\n",
    "        # mu = self.mu(hidden)\n",
    "        # mu = torch.tanh(mu)\n",
    "        # log_var = self.log_var(hidden)\n",
    "        # log_var = torch.tanh(log_var)\n",
    "        # y = self.reparameterize(mu, log_var)\n",
    "        # self.kl_loss = self.kl_loss_fun(mu, log_var)\n",
    "        \n",
    "        # --------------------------------- decoding --------------------------------- #\n",
    "        x = repeat(x, 'b f -> b t f', t=nTime)\n",
    "        x = self.encoder(x)\n",
    "        x = rearrange(x, 'b t f -> b f t')\n",
    "        x = self.dec_conv(x)\n",
    "        x = rearrange(x, 'b f t -> b t f')\n",
    "        return x\n",
    "        \n",
    "        \n",
    "class PL_model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(PL_model, self).__init__()\n",
    "        self.model = TrajNet()\n",
    "        self.c = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        self.train()\n",
    "        y = self.forward(batch)\n",
    "        loss = torch.nn.functional.mse_loss(y, batch[:, :, 0:2]) # + self.model.kl_loss * self.model.alpha\n",
    "        self.log('train_loss', loss)        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        \n",
    "    def training_epoch_end(self, training_step_outputs):        \n",
    "        if self.c % 50 == 0:\n",
    "            self.eval()\n",
    "            \n",
    "            # training set\n",
    "            x = dataset_train[0:1]\n",
    "            y = self.forward(torch.from_numpy(x).to(self.device).double())\n",
    "            x = x.squeeze()\n",
    "            y = y.squeeze()\n",
    "            y = y.detach().cpu().numpy()\n",
    "            ax[0].clear()\n",
    "            ax[0].plot(x[:, 0], x[:, 1], '-')\n",
    "            ax[0].plot(y[:, 0], y[:, 1], '-')\n",
    "            ax[0].plot(0, 0, 'or')\n",
    "            ax[0].axis('equal')\n",
    "            \n",
    "            # evaluation set\n",
    "            x = dataset_val[0:1]\n",
    "            y = self.forward(torch.from_numpy(x).to(self.device).double())\n",
    "            x = x.squeeze()\n",
    "            y = y.squeeze()\n",
    "            y = y.detach().cpu().numpy()\n",
    "            ax[1].clear()\n",
    "            ax[1].plot(x[:, 0], x[:, 1], '-')\n",
    "            ax[1].plot(y[:, 0], y[:, 1], '-')\n",
    "            ax[1].plot(0, 0, 'or')\n",
    "            ax[1].axis('equal')            \n",
    "                        \n",
    "            img = utils.Plot.fig2img(fig)\n",
    "            wandb_logger.log_image('traj', [img])\n",
    "            self.c = 0\n",
    "        self.c += 1\n",
    "          \n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "model = PL_model().double()\n",
    "trainer = pl.Trainer(max_epochs=10000, \n",
    "                     logger=wandb_logger, \n",
    "                     log_every_n_steps=10,\n",
    "                     accelerator='gpu', \n",
    "                     strategy='dp')\n",
    "trainer.fit(model, dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/acercyc/projects/Keio Testing_analysis/src/TrajNet_test_20220707_LSTM_res.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/acercyc/projects/Keio%20Testing_analysis/src/TrajNet_test_20220707_LSTM_res.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m m \u001b[39m=\u001b[39m TrajNet()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/acercyc/projects/Keio%20Testing_analysis/src/TrajNet_test_20220707_LSTM_res.ipynb#ch0000004vscode-remote?line=1'>2</a>\u001b[0m m\u001b[39m.\u001b[39;49mforward(torch\u001b[39m.\u001b[39;49mrand((\u001b[39m5\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m4\u001b[39;49m)))\n",
      "\u001b[1;32m/home/acercyc/projects/Keio Testing_analysis/src/TrajNet_test_20220707_LSTM_res.ipynb Cell 5'\u001b[0m in \u001b[0;36mTrajNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/acercyc/projects/Keio%20Testing_analysis/src/TrajNet_test_20220707_LSTM_res.ipynb#ch0000004vscode-remote?line=86'>87</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menc_conv(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/acercyc/projects/Keio%20Testing_analysis/src/TrajNet_test_20220707_LSTM_res.ipynb#ch0000004vscode-remote?line=87'>88</a>\u001b[0m x \u001b[39m=\u001b[39m rearrange(x, \u001b[39m'\u001b[39m\u001b[39mb f t -> b t f\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/acercyc/projects/Keio%20Testing_analysis/src/TrajNet_test_20220707_LSTM_res.ipynb#ch0000004vscode-remote?line=88'>89</a>\u001b[0m x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(x)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/acercyc/projects/Keio%20Testing_analysis/src/TrajNet_test_20220707_LSTM_res.ipynb#ch0000004vscode-remote?line=90'>91</a>\u001b[0m \u001b[39m# ----------------------------- hidden bottleneck ---------------------------- #\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blocalhost/home/acercyc/projects/Keio%20Testing_analysis/src/TrajNet_test_20220707_LSTM_res.ipynb#ch0000004vscode-remote?line=91'>92</a>\u001b[0m x \u001b[39m=\u001b[39m x[:, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "m = TrajNet()\n",
    "m.forward(torch.rand((5, 3, 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c38880c0b251f16d71f4d18036765822673f8ed835e0120528c8cebbbd5836f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
