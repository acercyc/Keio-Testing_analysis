{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33macercyc\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.19"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/acercyc/projects/Keio Testing_analysis/src/wandb/run-20220707_200406-AE_small_genOnly</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/acercyc/test_20220707/runs/AE_small_genOnly\" target=\"_blank\">AE_small_genOnly</a></strong> to <a href=\"https://wandb.ai/acercyc/test_20220707\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import utils \n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "from einops import rearrange, reduce, repeat\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(project=\"test_20220707\", id='AE_small_genOnly_xy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SpiralDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, nTime=72, nBatch=128, seed=0):\n",
    "        x = utils.SynthData.spiral(nTime, nBatch, seed)\n",
    "        x_, y_ = utils.DataProcessing.cart2pol(x[:, :, 0], x[:, :, 1])\n",
    "        x_ = repeat(x_, 'b t -> b t f', f=1)\n",
    "        y_ = repeat(y_, 'b t -> b t f', f=1)\n",
    "        x = np.concatenate([x, x_, y_], axis=2)\n",
    "        self.data = x\n",
    "              \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx, :, :]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "    \n",
    "    \n",
    "dataset_train = SpiralDataset(nTime=72, nBatch=128, seed=0)\n",
    "dataset_val = SpiralDataset(nTime=72, nBatch=128, seed=1)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_train, batch_size=128, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/acercyc/anaconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1814: PossibleUserWarning: GPU available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='gpu', devices=2)`.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name  | Type    | Params\n",
      "----------------------------------\n",
      "0 | model | TrajNet | 17.7 K\n",
      "----------------------------------\n",
      "17.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "17.7 K    Total params\n",
      "0.071     Total estimated model params size (MB)\n",
      "/home/acercyc/anaconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/acercyc/anaconda3/envs/py38/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1933: PossibleUserWarning: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=10). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae7fd525317348f19b52b805d322797a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class TrajNet(nn.Module):\n",
    "    def __init__(self, nHidden=8, nFeature=16, nhead=16, dim_feedforward=16, num_layers=4, dropout=0.1):\n",
    "        super(TrajNet, self).__init__()\n",
    "        self.nhidden = nHidden\n",
    "        self.nFeature = nFeature\n",
    "\n",
    "        # encoding\n",
    "        self.conv_enc = nn.Conv1d(5, nFeature, 1)\n",
    "        encoder_ = nn.TransformerEncoderLayer(d_model=nFeature,\n",
    "                                              nhead=nhead,\n",
    "                                              dim_feedforward=dim_feedforward,\n",
    "                                              batch_first=True, \n",
    "                                              activation='gelu',\n",
    "                                              dropout=dropout)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_, num_layers=num_layers)\n",
    "        \n",
    "        # hidden\n",
    "        self.hidden = nn.Linear(nFeature, nHidden)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.mu = nn.Linear(nFeature, nHidden)\n",
    "        self.log_var = nn.Linear(nFeature, nHidden)\n",
    "        self.alpha = torch.tensor(0.01)\n",
    "        \n",
    "        \n",
    "        # decode\n",
    "        self.conv_dec1 = nn.Conv1d(nHidden + 1, nFeature, 1)    \n",
    "        decoder_ = nn.TransformerEncoderLayer(d_model=nFeature, \n",
    "                                              nhead=nhead, \n",
    "                                              dim_feedforward=dim_feedforward, \n",
    "                                              batch_first=True, \n",
    "                                              activation='gelu',\n",
    "                                              dropout=dropout)\n",
    "        self.decoder = nn.TransformerEncoder(decoder_, num_layers=int(num_layers/2)) \n",
    "        generator_ = nn.TransformerEncoderLayer(d_model=nFeature, \n",
    "                                                nhead=nhead, \n",
    "                                                dim_feedforward=dim_feedforward, \n",
    "                                                batch_first=True, \n",
    "                                                activation='gelu',\n",
    "                                                dropout=dropout)\n",
    "        # generation   \n",
    "        self.generator = nn.TransformerEncoder(generator_, num_layers=int(num_layers))\n",
    "        self.conv_dec2 = nn.Conv1d(nFeature, 4, 1)              \n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def positionEncoding(x):\n",
    "        # x: b t f \n",
    "        nBatch = x.shape[0]\n",
    "        nTime = x.shape[1]\n",
    "        p = torch.arange(0, nTime).type_as(x)\n",
    "        p = p / 300\n",
    "        p = repeat(p, 't -> b t f', b=nBatch, f=1)\n",
    "        x = torch.concat([x, p], dim=2)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def reparameterize(mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)  # standard deviation\n",
    "        sample = torch.normal(mu, std).type_as(mu) # b f\n",
    "        return sample\n",
    "    \n",
    "    @staticmethod\n",
    "    def kl_loss_fun(mu, log_var):\n",
    "        return (-0.5 * (1 + log_var - mu**2 - torch.exp(log_var)).sum(dim=1)).mean(dim=0)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: b t f \n",
    "        nBatch = x.shape[0]\n",
    "        nTime = x.shape[1]\n",
    "        \n",
    "        # --------------------------------- encoding --------------------------------- #\n",
    "        # expend features\n",
    "        x = self.positionEncoding(x)\n",
    "        x = rearrange(x, 'b t f -> b f t')\n",
    "        x = self.conv_enc(x)\n",
    "        x = rearrange(x, 'b f t -> b t f')\n",
    "        \n",
    "        # ----------------------------- hidden bottleneck ---------------------------- #\n",
    "        hidden = x[:, 0, :]\n",
    "        hidden = self.hidden(hidden)\n",
    "        hidden = torch.tanh(hidden)\n",
    "        # hidden = self.silu(hidden)\n",
    "        # mu = self.mu(hidden)\n",
    "        # mu = torch.tanh(mu)\n",
    "        # log_var = self.log_var(hidden)\n",
    "        # log_var = torch.tanh(log_var)\n",
    "        # y = self.reparameterize(mu, log_var)\n",
    "        # self.kl_loss = self.kl_loss_fun(mu, log_var)\n",
    "        \n",
    "        # --------------------------------- decoding --------------------------------- #\n",
    "        y = repeat(hidden, 'b f -> b t f', t=nTime)\n",
    "        y = self.positionEncoding(y)\n",
    "        y = rearrange(y, 'b t f -> b f t')\n",
    "        y = self.conv_dec1(y)\n",
    "        y = rearrange(y, 'b f t -> b t f')\n",
    "        # y = self.decoder(y)\n",
    "        mask = nn.Transformer.generate_square_subsequent_mask(nTime).type_as(y)       \n",
    "        y = self.generator(y, mask=mask)\n",
    "        y = rearrange(y, 'b t f -> b f t')\n",
    "        y = self.conv_dec2(y)\n",
    "        y = rearrange(y, 'b f t -> b t f')\n",
    "        return y \n",
    "        \n",
    "        \n",
    "class PL_model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super(PL_model, self).__init__()\n",
    "        self.model = TrajNet()\n",
    "        self.c = 0\n",
    "       \n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        y = self.forward(batch)\n",
    "        loss = torch.nn.functional.mse_loss(y[:, :, 0:2], batch[:, :, 0:2]) # + self.model.kl_loss * self.model.alpha\n",
    "        self.log('train_loss', loss)        \n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.003)\n",
    "        \n",
    "    def training_epoch_end(self, training_step_outputs):        \n",
    "        if self.c % 50 == 0:\n",
    "            self.eval()\n",
    "            \n",
    "            # training set\n",
    "            x = dataset_train[0:1]\n",
    "            self.eval()\n",
    "            y = self.forward(torch.from_numpy(x).double())\n",
    "            x = x.squeeze()\n",
    "            y = y.squeeze()\n",
    "            y = y.detach().numpy()\n",
    "            ax[0].clear()\n",
    "            ax[0].plot(x[:, 0], x[:, 1], '-')\n",
    "            ax[0].plot(y[:, 0], y[:, 1], '-')\n",
    "            ax[0].plot(0, 0, 'or')\n",
    "            ax[0].axis('equal')\n",
    "            \n",
    "            # evaluation set\n",
    "            x = dataset_val[0:1]\n",
    "            y = self.forward(torch.from_numpy(x).double())\n",
    "            x = x.squeeze()\n",
    "            y = y.squeeze()\n",
    "            y = y.detach().numpy()\n",
    "            ax[1].clear()\n",
    "            ax[1].plot(x[:, 0], x[:, 1], '-')\n",
    "            ax[1].plot(y[:, 0], y[:, 1], '-')\n",
    "            ax[1].plot(0, 0, 'or')\n",
    "            ax[1].axis('equal')            \n",
    "                        \n",
    "            img = utils.Plot.fig2img(fig)\n",
    "            wandb_logger.log_image('traj', [img])\n",
    "            self.c = 0\n",
    "        self.c += 1\n",
    "          \n",
    "# %matplotlib qt\n",
    "\n",
    "# x = torch.randn((3, 4, 2)).type(model.dtype)\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "model = PL_model().double()\n",
    "trainer = pl.Trainer(max_epochs=10000, logger=wandb_logger, log_every_n_steps=10)\n",
    "trainer.fit(model, dataloader_train)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c38880c0b251f16d71f4d18036765822673f8ed835e0120528c8cebbbd5836f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
